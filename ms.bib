@misc{layernorm2016,
  note = {Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton. \newblock Layer normalization. \newblock {\em arXiv preprint arXiv:1607.06450}, 2016.}
}

@misc{bahdanau2014neural,
  note = {Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \newblock Neural machine translation by jointly learning to align and translate. \newblock {\em CoRR}, abs/1409.0473, 2014.}
}

@misc{DBLP:journals/corr/BritzGLL17,
  note = {Denny Britz, Anna Goldie, Minh{-}Thang Luong, and Quoc~V. Le. \newblock Massive exploration of neural machine translation architectures. \newblock {\em CoRR}, abs/1703.03906, 2017.}
}

@misc{cheng2016long,
  note = {Jianpeng Cheng, Li~Dong, and Mirella Lapata. \newblock Long short-term memory-networks for machine reading. \newblock {\em arXiv preprint arXiv:1601.06733}, 2016.}
}

@misc{cho2014learning,
  note = {Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. \newblock Learning phrase representations using rnn encoder-decoder for statistical machine translation. \newblock {\em CoRR}, abs/1406.1078, 2014.}
}

@misc{xception2016,
  note = {Francois Chollet. \newblock Xception: Deep learning with depthwise separable convolutions. \newblock {\em arXiv preprint arXiv:1610.02357}, 2016.}
}

@misc{gruEval14,
  note = {Junyoung Chung, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho, and Yoshua Bengio. \newblock Empirical evaluation of gated recurrent neural networks on sequence modeling. \newblock {\em CoRR}, abs/1412.3555, 2014.}
}

@misc{dyer-rnng:16,
  note = {Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah~A. Smith. \newblock Recurrent neural network grammars. \newblock In {\em Proc. of NAACL}, 2016.}
}

@misc{JonasFaceNet2017,
  note = {Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N. Dauphin. \newblock Convolutional sequence to sequence learning. \newblock {\em arXiv preprint arXiv:1705.03122v2}, 2017.}
}

@misc{graves2013generating,
  note = {Alex Graves. \newblock Generating sequences with recurrent neural networks. \newblock {\em arXiv preprint arXiv:1308.0850}, 2013.}
}

@misc{he2016deep,
  note = {Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \newblock Deep residual learning for image recognition. \newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 770--778, 2016.}
}

@misc{hochreiter2001gradient,
  note = {Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J{\"u}rgen Schmidhuber. \newblock Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.}
}

@misc{hochreiter1997,
  note = {Sepp Hochreiter and J{\"u}rgen Schmidhuber. \newblock Long short-term memory. \newblock {\em Neural computation}, 9(8):1735--1780, 1997.}
}

@misc{huang-harper:2009:EMNLP,
  note = {Zhongqiang Huang and Mary Harper. \newblock Self-training {PCFG} grammars with latent annotations across languages. \newblock In {\em Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing}, pages 832--841. ACL, August 2009.}
}

@misc{jozefowicz2016exploring,
  note = {Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. \newblock Exploring the limits of language modeling. \newblock {\em arXiv preprint arXiv:1602.02410}, 2016.}
}

@misc{extendedngpu,
  note = {{\L}ukasz Kaiser and Samy Bengio. \newblock Can active memory replace attention? \newblock In {\em Advances in Neural Information Processing Systems, ({NIPS})}, 2016.}
}

@misc{neural_gpu,
  note = {\L{}ukasz Kaiser and Ilya Sutskever. \newblock Neural {GPU}s learn algorithms. \newblock In {\em International Conference on Learning Representations ({ICLR})}, 2016.}
}

@misc{NalBytenet2017,
  note = {Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van~den Oord, Alex Graves, and Koray Kavukcuoglu. \newblock Neural machine translation in linear time. \newblock {\em arXiv preprint arXiv:1610.10099v2}, 2017.}
}

@misc{structuredAttentionNetworks,
  note = {Yoon Kim, Carl Denton, Luong Hoang, and Alexander~M. Rush. \newblock Structured attention networks. \newblock In {\em International Conference on Learning Representations}, 2017.}
}

@misc{kingma2014adam,
  note = {Diederik Kingma and Jimmy Ba. \newblock Adam: A method for stochastic optimization. \newblock In {\em ICLR}, 2015.}
}

@misc{Kuchaiev2017Factorization,
  note = {Oleksii Kuchaiev and Boris Ginsburg. \newblock Factorization tricks for {LSTM} networks. \newblock {\em arXiv preprint arXiv:1703.10722}, 2017.}
}

@misc{lin2017structured,
  note = {Zhouhan Lin, Minwei Feng, Cicero Nogueira~dos Santos, Mo~Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. \newblock A structured self-attentive sentence embedding. \newblock {\em arXiv preprint arXiv:1703.03130}, 2017.}
}

@misc{multiseq2seq,
  note = {Minh-Thang Luong, Quoc~V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. \newblock Multi-task sequence to sequence learning. \newblock {\em arXiv preprint arXiv:1511.06114}, 2015.}
}

@misc{luong2015effective,
  note = {Minh-Thang Luong, Hieu Pham, and Christopher~D Manning. \newblock Effective approaches to attention-based neural machine translation. \newblock {\em arXiv preprint arXiv:1508.04025}, 2015.}
}

@misc{marcus1993building,
  note = {Mitchell~P Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini. \newblock Building a large annotated corpus of english: The penn treebank. \newblock {\em Computational linguistics}, 19(2):313--330, 1993.}
}

@misc{mcclosky-etAl:2006:NAACL,
  note = {David McClosky, Eugene Charniak, and Mark Johnson. \newblock Effective self-training for parsing. \newblock In {\em Proceedings of the Human Language Technology Conference of the NAACL, Main Conference}, pages 152--159. ACL, June 2006.}
}

@misc{decomposableAttnModel,
  note = {Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. \newblock A decomposable attention model. \newblock In {\em Empirical Methods in Natural Language Processing}, 2016.}
}

@misc{paulus2017deep,
  note = {Romain Paulus, Caiming Xiong, and Richard Socher. \newblock A deep reinforced model for abstractive summarization. \newblock {\em arXiv preprint arXiv:1705.04304}, 2017.}
}

@misc{petrov-EtAl:2006:ACL,
  note = {Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. \newblock Learning accurate, compact, and interpretable tree annotation. \newblock In {\em Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL}, pages 433--440. ACL, July 2006.}
}

@misc{press2016using,
  note = {Ofir Press and Lior Wolf. \newblock Using the output embedding to improve language models. \newblock {\em arXiv preprint arXiv:1608.05859}, 2016.}
}

@misc{sennrich2015neural,
  note = {Rico Sennrich, Barry Haddow, and Alexandra Birch. \newblock Neural machine translation of rare words with subword units. \newblock {\em arXiv preprint arXiv:1508.07909}, 2015.}
}

@misc{shazeer2017outrageously,
  note = {Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. \newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. \newblock {\em arXiv preprint arXiv:1701.06538}, 2017.}
}

@misc{srivastava2014dropout,
  note = {Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \newblock Dropout: a simple way to prevent neural networks from overfitting. \newblock {\em Journal of Machine Learning Research}, 15(1):1929--1958, 2014.}
}

@misc{sukhbaatar2015,
  note = {Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. \newblock End-to-end memory networks. \newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems 28}, pages 2440--2448. Curran Associates, Inc., 2015.}
}

@misc{sutskever14,
  note = {Ilya Sutskever, Oriol Vinyals, and Quoc~VV Le. \newblock Sequence to sequence learning with neural networks. \newblock In {\em Advances in Neural Information Processing Systems}, pages 3104--3112, 2014.}
}

@misc{DBLP:journals/corr/SzegedyVISW15,
  note = {Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. \newblock Rethinking the inception architecture for computer vision. \newblock {\em CoRR}, abs/1512.00567, 2015.}
}

@misc{KVparse15,
  note = {{Vinyals {\&} Kaiser}, Koo, Petrov, Sutskever, and Hinton. \newblock Grammar as a foreign language. \newblock In {\em Advances in Neural Information Processing Systems}, 2015.}
}

@misc{wu2016google,
  note = {Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et~al. \newblock Google's neural machine translation system: Bridging the gap between human and machine translation. \newblock {\em arXiv preprint arXiv:1609.08144}, 2016.}
}

@misc{DBLP:journals/corr/ZhouCWLX16,
  note = {Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. \newblock Deep recurrent models with fast-forward connections for neural machine translation. \newblock {\em CoRR}, abs/1606.04199, 2016.}
}

@misc{zhu-EtAl:2013:ACL,
  note = {Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.
          \newblock Fast and accurate shift-reduce constituent parsing.
          \newblock In {\em Proceedings of the 51st Annual Meeting of the ACL (Volume 1:
          Long Papers)}, pages 434--443. ACL, August 2013.}
}
